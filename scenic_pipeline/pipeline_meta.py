"""
=================
pySCENIC metapipeline
=================

Author: Lucy Garner
  modifications : Aaron M. Allen

Overview
========
This pipeline performs pySCENIC meta analysis

Performs the following steps of pySCENIC (https://pyscenic.readthedocs.io/en/latest/index.html) analysis:
1. Deriving co-expression modules
2. Finding enriched motifs and corresponding target genes for modules
3. Quantifying activity of gene signatures/regulons across single cells

pySCENIC steps are run 100 times and regulons are aggregated to identify the
most stable regulons and their most common features.

Usage
=====

See https://cgat-core.readthedocs.io/en/latest/ for how to use CGAT-core pipelines

Configuration
-------------
The pipeline requires a pipeline.yml configuration file

Default configuration files can be generated by executing:
    python pipeline_pyscenic.py config

Input files
-----------
Four files as follows:

1. Raw or normalised expression matrix (CSV) named *_raw-expression.csv/*_normalised-expression.csv
Genes as rows, cells as columns
Should have been filtered to remove low quality cells and doublets
Inside a data.dir directory

2. A list of TFs from the genome under study
(inside resources.dir directory; tfs_list in pipeline.yml)

3. Databases ranking the whole genome of your species of interest based on
regulatory features (i.e. transcription factors)
Ranking databases are typically stored in the feather format and can be
downloaded from cisTargetDBs
(inside resources.dir directory; database_fname in pipeline.yml)

4. Motif annotation database providing the missing link between an enriched motif
and the transcription factor that binds this motif
This pipeline needs a TSV text file where every line represents a particular annotation
(inside resources.dir directory; annotations in pipeline.yml)

Dependencies
------------
This pipeline requires: cgatcore, pyscenic and dependencies

Pipeline output
===============

adjacencies.csv    # gives strength of evidence for association between TF and targets
adjacencies_wCor.csv    # gives correlation to the associations between TF and targets
reg.csv    # modules of TFs and their target genes (not been refined to regulons)
aucell.csv   # activity of identified gene regulatory modules within each cell

"""

from ruffus import *
import sys
import os
from cgatcore import pipeline as P
import re

PARAMS = P.get_parameters(
    ["%s/pipeline.yml" % os.path.splitext(__file__)[0],
      "../pipeline.yml",
      "pipeline.yml"])

@transform("data.dir/*_*-expression.csv",
           regex(r"data.dir/([^_]+)_(r.*|n.*)-expression.csv"),
           r"filtered_expression.dir/\2.dir/\1.dir/filtered-expression.csv")
def gene_filtering(infile, outfile):
    '''
    Filtering of the input expression matrix to remove genes expressed in few cells
    '''

    sample = infile.split('/')[1]
    sample = sample.split('_')[0]

    PY_PATH = os.path.join(os.getcwd(), "python")

    statement = """python %(PY_PATH)s/filter_csv.py
                --input %(infile)s --umi_counts %(filtering_UMI_counts)s
                --min_percent %(filtering_min_percent)s
                --output %(outfile)s
                """

    P.run(statement, job_options="-t 024:00:00",
          job_threads = PARAMS["filtering_threads"],
          job_memory = PARAMS["filtering_memory"],
          job_queue = PARAMS["cluster_queue"],
          job_condaenv = PARAMS["pyscenic_conda_env"])

@follows(mkdir("temp.dir"))
@subdivide(gene_filtering,
           regex("filtered_expression.dir/(\w+).dir/([A-Za-z0-9-]+).dir/filtered-expression.csv"),
           r"temp.dir/\1_\2_*.tmp")
def dummy(infile, outfile):
    '''
    Dummy function to generate inputs for running pySCENIC 100 times
    '''

    sample_name = infile.split("/")[2].replace(".dir", "")
    data_type = infile.split("/")[1].replace(".dir", "")

    for i in range(1,101):
        open("temp.dir/" + data_type + "_" + sample_name + "_" +
             str(i) + ".tmp", "a").close()

@jobs_limit(100)        
@follows(mkdir("results.dir"))
@transform(dummy, regex(r"temp.dir/(\w+)_([A-Za-z0-9-]+)_(\d+).tmp"),
           r"results.dir/run_\3.dir/\1.dir/\2.dir/aucell.csv")
def pyscenic_run(infile, outfile):
    '''
    Run pySCENIC X times
    '''
    sample_name = infile.split("/")[1].split("_")[1]
    data_type = infile.split("/")[1].split("_")[0]
    run_number = infile.split("/")[1].split("_")[2].replace(".tmp", "")

    statement = """mkdir -p results.dir/run_%(run_number)s.dir &&
                cd results.dir/run_%(run_number)s.dir &&
                python ../../pipeline_pyscenic.py --cluster-queue-manager=slurm
                --cluster-queue=long make full &&
                echo "pipeline_pyscenic.py has completed"
                """

    P.run(statement, job_options="-t 168:00:00",
          job_threads = PARAMS["pyscenic_threads"],
          job_memory = PARAMS["pyscenic_memory"],
          job_queue = PARAMS["cluster_queue"],
          job_condaenv = PARAMS["pyscenic_conda_env"],
          ignore_errors = True)

@follows(pyscenic_run)
@transform("results.dir/run_*.dir/*.dir/*.dir/reg.csv",
           regex(r"results.dir/run_(\d+).dir/(r.*|n.*).dir/([^_]+).dir/reg.csv"),
           r"results.dir/run_\1.dir/\2.dir/\3.dir/regulons.csv")
def generate_regulons(infile, outfile):
    '''
    Generate regulons from TF modules
    '''

    sample = infile.split("/")[3]
    sample = sample.replace(".dir", "")
    run_number = infile.split("/")[1]
    infile = re.split("\d+.dir/", infile)[1]

    statement = """cd results.dir/%(run_number)s &&
                python ../../python/generate_regulons.py
                --sample %(sample)s
                --ctx_output %(infile)s
                """

    P.run(statement, job_options="-t 024:00:00",
          job_threads = PARAMS["pyscenic_threads"],
          job_memory = PARAMS["pyscenic_memory"],
          job_queue = PARAMS["cluster_queue"],
          job_condaenv = PARAMS["pyscenic_conda_env"])

@transform(pyscenic_run,
           regex(r"results.dir/run_(\d+).dir/(r.*|n.*).dir/([^_]+).dir/aucell.csv"),
           r"results.dir/run_\1.dir/\2.dir/\3.dir/aucell_thresholds.csv")
def regulon_binarization(infile, outfile):
    '''
    Binarize the regulons
    '''

    data_type = infile.split("/")[2]
    sample = infile.split("/")[3]
    run_number = infile.split("/")[1]
    infile = infile.split("/")[4]

    if PARAMS["binarize_custom_aucell_thresholds"] == None:
        custom_aucell_thresholds = ""
    else:
        custom_aucell_thresholds = "--custom_auc_thresholds " + PARAMS["binarize_custom_aucell_thresholds"]

    statement = """cd results.dir/%(run_number)s/%(data_type)s/%(sample)s &&
                mkdir -p plots.dir &&
                python ../../../../python/regulon_binarization.py
                --binarize_threads %(binarize_threads)s
                --aucell_output %(infile)s -t
                %(custom_aucell_thresholds)s
                """

    P.run(statement, job_options="-t 024:00:00",
          job_threads = PARAMS["binarize_threads"],
          job_memory = PARAMS["binarize_memory"],
          job_queue = PARAMS["cluster_queue"],
          job_condaenv = PARAMS["pyscenic_conda_env"],
          ignore_errors = True)

@transform(pyscenic_run,
           regex(r"results.dir/run_(\d+).dir/(r.*|n.*).dir/([^_]+).dir/aucell.csv"),
           add_inputs(r"filtered_expression.dir/\2.dir/\3.dir/filtered-expression.csv"),
           r"results.dir/run_\1.dir/\2.dir/\3.dir/aucell_zscores.csv")
def rss_zscore(infiles, outfile):
    '''
    Calculate the RSS (regulon specificity score) Z-score for a regulon
    within a given annotation e.g. cell cluster
    '''

    sample = infiles[0].split("/")[3].replace(".dir", "")
    run_number = infiles[0].split("/")[1]
    exp_mtx = "../../" + infiles[1]
    infile = infiles[0].replace("results.dir/" + run_number + "/", "")

    statement = """cd results.dir/%(run_number)s &&
                python ../../python/rss_zscore.py
                --sample %(sample)s
                --exp_mtx %(exp_mtx)s
                --aucell_output %(infile)s
                --annotation_input %(rss_zscore_annotation_input)s -t
                """

    P.run(statement, job_options="-t 024:00:00",
          job_threads = PARAMS["rss_zscore_threads"],
          job_memory = PARAMS["rss_zscore_memory"],
          job_queue = PARAMS["cluster_queue"],
          job_condaenv = PARAMS["pyscenic_conda_env"])

@follows(regulon_binarization)
@transform(pyscenic_run,
           regex(r"results.dir/run_(\d+).dir/(r.*|n.*).dir/([^_]+).dir/aucell.csv"),
           add_inputs(r"filtered_expression.dir/\2.dir/\3.dir/filtered-expression.csv"),
           r"results.dir/run_\1.dir/\2.dir/\3.dir/plots.dir/aucell_heatmap.png")
def aucell_heatmap(infiles, outfile):
    '''
    Plot a heatmap of AUCell scores
    '''

    sample = infiles[0].split("/")[3].replace(".dir", "")
    run_number = infiles[0].split("/")[1]
    exp_mtx = "../../" + infiles[1]
    infile = infiles[0].replace("results.dir/" + run_number + "/", "")

    statement = """cd results.dir/%(run_number)s &&
                python ../../python/aucell_heatmap.py
                --sample %(sample)s
                --exp_mtx %(exp_mtx)s
                --aucell_output %(infile)s -t
                """

    P.run(statement, job_options="-t 024:00:00",
          job_threads = PARAMS["aucell_heatmap_threads"],
          job_memory = PARAMS["aucell_heatmap_memory"],
          job_queue = PARAMS["cluster_queue"],
          job_condaenv = PARAMS["pyscenic_conda_env"])

@follows(pyscenic_run, generate_regulons, rss_zscore, aucell_heatmap)
def individual_runs():
    pass

@collate(pyscenic_run,
         regex(r"results.dir/run_\d+.dir/(r.*|n.*).dir/([^_]+).dir/aucell.csv"),
         r"results.dir/aggregated.dir/\1.dir/\2.dir/high_confidence_regulons.csv")
def high_confidence_regulons(infiles, outfile):
    '''
    Generate high confidence regulons
    Regulons that are identified in > n% of runs
    Genes within regulons that are found in > n% of runs
    '''

    file_path = infiles[0].split("/")[0]
    data_type = infiles[0].split("/")[2].replace(".dir", "")
    sample = infiles[0].split("/")[3].replace(".dir", "")

    statement = """Rscript R/high_confidence_regulons.R
                --file_path %(file_path)s
                --sample_name %(sample)s
                --data_type %(data_type)s
                --number_runs %(pyscenic_number_runs)s
                --number_genes %(high_conf_reg_number_genes)s
                --percent_occurance %(high_conf_reg_percent_occurance)s
                """

    P.run(statement, job_options="-t 001:00:00",
          job_threads = PARAMS["high_conf_reg_threads"],
          job_memory = PARAMS["high_conf_reg_memory"],
          job_queue = PARAMS["cluster_queue"],
          job_condaenv = PARAMS["pyscenic_r_conda_env"])

@transform(high_confidence_regulons,
           regex(r"results.dir/aggregated.dir/(r.*|n.*).dir/([^_]+).dir/high_confidence_regulons.csv"),
           r"results.dir/aggregated.dir/\1.dir/\2.dir/aucell.csv")
def aggregated_aucell(infile, outfile):
    '''
    Calculate AUCell scores for high confidence regulons
    '''

    data_type = infile.split("/")[2].replace(".dir", "")
    sample = infile.split("/")[3].replace(".dir", "")
    filtered_expression = "filtered_expression.dir/" + data_type + ".dir/" + sample + ".dir/filtered-expression.csv"
    auc_threshold = PARAMS["calculate_aucell_threshold"]

    statement = """Rscript R/calculate_aucell.R
                --sample_name %(sample)s
                --data_type %(data_type)s
                --seed %(calculate_aucell_seed)s
                --filtered_expression %(filtered_expression)s
                --auc_threshold %(auc_threshold)s
                """

    P.run(statement, job_options="-t 024:00:00",
          job_threads = PARAMS["calculate_aucell_threads"],
          job_memory = PARAMS["calculate_aucell_memory"],
          job_queue = PARAMS["cluster_queue"],
          job_condaenv = PARAMS["pyscenic_r_conda_env"])

@transform(aggregated_aucell,
           regex(r"results.dir/aggregated.dir/(r.*|n.*).dir/([^_]+).dir/aucell.csv"),
           r"results.dir/aggregated.dir/\1.dir/\2.dir/aucell_thresholds.csv")
def aggregated_regulon_binarization(infile, outfile):
    '''
    Binarize the high confidence regulons
    '''

    data_type = infile.split("/")[2]
    sample = infile.split("/")[3]
    infile = infile.split("/")[4]

    if PARAMS["binarize_custom_aucell_thresholds"] == None:
        custom_aucell_thresholds = ""
    else:
        custom_aucell_thresholds = "--custom_auc_thresholds " + PARAMS["binarize_custom_aucell_thresholds"]

    statement = """cd results.dir/aggregated.dir/%(data_type)s/%(sample)s &&
                mkdir -p plots.dir &&
                python ../../../../python/regulon_binarization.py
                --binarize_threads %(binarize_threads)s
                --aucell_output %(infile)s -t
                """

    P.run(statement, job_options="-t 024:00:00",
          job_threads = PARAMS["binarize_threads"],
          job_memory = PARAMS["binarize_memory"],
          job_queue = PARAMS["cluster_queue"],
          job_condaenv = PARAMS["pyscenic_conda_env"],
          ignore_errors = True)

@transform(aggregated_aucell,
           regex(r"results.dir/aggregated.dir/(r.*|n.*).dir/([^_]+).dir/aucell.csv"),
           add_inputs(r"filtered_expression.dir/\1.dir/\2.dir/filtered-expression.csv"),
           r"results.dir/aggregated.dir/\1.dir/\2.dir/aucell_zscores.csv")
def aggregated_rss_zscore(infiles, outfile):
    '''
    Calculate the RSS (regulon specificity score) Z-score for a regulon
    within a given annotation e.g. cell cluster
    '''

    sample = infiles[0].split("/")[3].replace(".dir", "")
    exp_mtx = "../../" + infiles[1]
    infile = infiles[0].replace("results.dir/aggregated.dir/", "")

    statement = """cd results.dir/aggregated.dir &&
                python ../../python/rss_zscore.py
                --sample %(sample)s
                --exp_mtx %(exp_mtx)s
                --aucell_output %(infile)s
                --annotation_input %(rss_zscore_annotation_input)s -t
                """

    P.run(statement, job_options="-t 024:00:00",
          job_threads = PARAMS["rss_zscore_threads"],
          job_memory = PARAMS["rss_zscore_memory"],
          job_queue = PARAMS["cluster_queue"],
          job_condaenv = PARAMS["pyscenic_conda_env"])

@transform(aggregated_aucell,
           regex(r"results.dir/aggregated.dir/(r.*|n.*).dir/([^_]+).dir/aucell.csv"),
           add_inputs(r"filtered_expression.dir/\1.dir/\2.dir/filtered-expression.csv"),
           r"results.dir/aggregated.dir/\1.dir/\2.dir/plots.dir/aucell_heatmap.png")
def aggregated_aucell_heatmap(infiles, outfile):
    '''
    Plot a heatmap of AUCell scores
    '''

    sample = infiles[0].split("/")[3].replace(".dir", "")
    exp_mtx = "../../" + infiles[1]
    infile = infiles[0].replace("results.dir/aggregated.dir/", "")

    statement = """cd results.dir/aggregated.dir &&
                python ../../python/aucell_heatmap.py
                --sample %(sample)s
                --exp_mtx %(exp_mtx)s
                --aucell_output %(infile)s -t
                """

    P.run(statement, job_options="-t 024:00:00",
          job_threads = PARAMS["aucell_heatmap_threads"],
          job_memory = PARAMS["aucell_heatmap_memory"],
          job_queue = PARAMS["cluster_queue"],
          job_condaenv = PARAMS["pyscenic_conda_env"])

@follows(aggregated_regulon_binarization, aggregated_rss_zscore,
         aggregated_aucell_heatmap)
def aggregated():
    pass

@follows(aggregated_rss_zscore)
@transform(aggregated_aucell,
           regex(r"results.dir/aggregated.dir/(r.*|n.*).dir/([^_]+).dir/aucell.csv"),
           r"reports.dir/\1.dir/\2.dir/scenic_analysis_R.html")
def pyscenic_r(infile, outfile):
    '''
    Run R analysis on high confidence regulons
    '''

    statement = """python pipeline_pyscenic_r.py
                --cluster-queue-manager=slurm
                --cluster-queue=long make full &&
                echo "pipeline_pyscenic_r.py has completed"
                """

    P.run(statement, job_options="-t 024:00:00",
          job_threads = PARAMS["pyscenic_r_threads"],
          job_memory = PARAMS["pyscenic_r_memory"],
          job_queue = PARAMS["cluster_queue"],
          job_condaenv = PARAMS["pyscenic_r_conda_env"],
          ignore_errors = True)

@follows(individual_runs, aggregated_regulon_binarization,
         aggregated_rss_zscore, aggregated_aucell_heatmap, pyscenic_r)
def full():
    pass

def main(argv = None):
    if argv is None:
        argv = sys.argv
    P.main(argv)

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
